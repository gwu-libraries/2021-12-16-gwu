{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBZaVkRgPdmy"
   },
   "source": [
    "# Library Carpentry: Tools for Humanists\n",
    "\n",
    "## Python Lesson\n",
    "\n",
    "### Part 2: Working with data using Pandas\n",
    "\n",
    "This lesson is adapted from Melanie Walsh's [Introduction to Cultural Analytics with Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/03-Data-Analysis/01-Pandas-Basics-Part1.html) and Dolsy Smith's pandas lesson taught in [Python Camp](https://libguides.gwu.edu/python-camp). \n",
    "\n",
    "We're going to learn some ways to work with tabular data (such as in a CSV file) to do exploration, analysis, and plotting with Python. We'll be using a particular Python library called pandas. The name pandas comes from \"Python Data Analysis\" library. \n",
    "\n",
    "We will cover how to:\n",
    "\n",
    "* Import Pandas\n",
    "* Read in a CSV file\n",
    "* Explore and filter data\n",
    "* Make simple plots and data visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esPqPknrmChh"
   },
   "source": [
    "The pandas library is already installed in the Google Colab environment, so we don't need to do anything special to install the module. We do, however, need to use the import command so that the Python interpreter will understand what we are referencing it in our code. \n",
    "\n",
    "We're going to import it and use a nickname, or shorter-to-type name for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tToCDiSPg3g"
   },
   "outputs": [],
   "source": [
    " import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoG_6PpAm4OV"
   },
   "source": [
    "Pandas is very handy at reading in tabular data from CSV or Excel and turning it into a Python data structure we can use for analysis. \n",
    "\n",
    "First, we need to load the CSV into this Google Colab environment. \n",
    "\n",
    "We will use the function `pd.read_csv()` to load our CSV file from an external site, providing the function with the URL. \n",
    "\n",
    "This creates a Pandas DataFrame object — often you'll see people use df as a variable name to represent the DataFrame, but you can name it anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hox_-lKPh5n"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/gwu-libraries/2022-07-14-gwu/gh-pages/files/Cleaned-columbian-college-volumes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcfKIDXmS1Op"
   },
   "source": [
    "We can see that this is a new Python data type, a DataFrame, which we got when we imported the pandas library. \n",
    "\n",
    "A DataFrame looks and acts a lot like a spreadsheet. \n",
    "\n",
    "Not all the rows are shown, but we can see at the bottom that there are 459. We also have column labels across the top, and index labels down the left-hand side. Here the index labels correspond to the row numbers (as in a spreadsheet, with the notable difference of starting with 0) but that doesn’t have to be the case. \n",
    "\n",
    "Note that those numbers weren't part of our spreadsheet that we imported.\n",
    "\n",
    "A DataFrame, like almost everything in Python, has a data type. But unlike a list or dictionary, a DataFrame is a user-defined type, meaning that the creators of the pandas library gave it special methods and properties we can use when working with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rf4UEiakSnY7"
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZU1ma5UTgtO"
   },
   "source": [
    "We can also take a look at some of the data using a method belonging to the DataFrame object called head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z28YhqLqTbZC"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KtsIOAD4g6_"
   },
   "source": [
    "\n",
    "The head method by default only shows us the first five rows, although we can change that by providing the number of rows as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5mmmiowZL5S"
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HmVW4iraG-R"
   },
   "source": [
    "You're probably wondering what all of those NaN values are in many of the columns. That's a missing value in the data, and pandas uses the Python \"Not a Number\" value (it's technically actually a float) to act as a placeholder in the data. It's a null value. \n",
    "\n",
    "Determining how to deal with missing values is an important step in working with any data. Let's first look at how many missing values are in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW4Y6Ck8auGM"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBpyILDRbwww"
   },
   "source": [
    "You can see that while there are 459 rows of data, some of the columns have fewer than 459 values. The remaining values are the NaN null values. We can see that the Series, Relation, and Medium Type columns are nearly or completely empty in this dataset. We'll get rid of those columns to make our data easier to work with (and smaller, which could be helpful for much larger datasets). \n",
    "\n",
    "Note also the Dtype column. That shows us the \"type\" of the column, much like a column in an Excel sheet has a type. In this case, \"object\" refers to strings or a column that has a mix of strings and numeric types. This is common with text data like our dataset. However you might also encounter datetime values or booleans or floats. \n",
    "\n",
    "See “[Working with missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html?highlight=nan/)” for more information in the pandas docs.\n",
    "\n",
    "Here's another way to look at missing data: we can count the number of null values (instead of non-null). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrwKMcs8dTuG"
   },
   "outputs": [],
   "source": [
    "#df.isnull() combining isnull() (returns True/False) with sum() gives us the number of null instead of the number of values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM87iIehem5f"
   },
   "source": [
    "Let's remove those columns that aren't useful to us because they're practically all null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7ortsFDeqtf"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Series\", \"Relation\", \"Medium Type\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKzx9SmlmS09"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJs9BTrYmceU"
   },
   "source": [
    "**Optional:**\n",
    "\n",
    "There are ways to replace NaN values with a different value, such as 0 or a string \"Unknown\". `.fillna()`\n",
    "\n",
    "`fillna()` operates on a column (or the entire dataframe) and takes as its first argument the value you want to have replace NaN. Then since we want to overwrite NaN in the DataFrame--change it in a way that persists rather than just outputting it, we add the keyword argument `inplace=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAqlZcytmklI"
   },
   "outputs": [],
   "source": [
    "df[\"Edition\"].fillna(\"Unknown\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6evGlGbpguKJ"
   },
   "source": [
    "**Subsets of data** \n",
    "\n",
    "There are some ways to look at subsets of our data either by position in the DataFrame (like certain row numbers) or by criteria. \n",
    "\n",
    "We're going to use notation that looks a lot like slicing, in combination with keyword `loc`. It's not really a method. Loc lets us use the numeric position of a row. \n",
    "\n",
    "There's a notable difference between how loc works and how slicing works. Loc is *inclusive*, meaning that the last position is included in the slice. That is not how slicing works (as you saw on strings and lists). \n",
    "\n",
    "(Note we are using the row positions, which happen to be same as the index here, but it's not always!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaD0MoRMR-9i"
   },
   "outputs": [],
   "source": [
    "df.loc[5:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmPa9CdpiIIq"
   },
   "source": [
    "We can select columns using their name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zOjSHl-TgVf"
   },
   "outputs": [],
   "source": [
    "# How to select columns\n",
    "subjects = df[\"Subject\"]\n",
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE9S3fM3lyXK"
   },
   "source": [
    "This returns a pandas Series. There is a numeric index for the values in the Series. \n",
    "\n",
    "We can combine column selection and using loc to get a particular or set of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNxdS3z6T8BV"
   },
   "outputs": [],
   "source": [
    "# see full value of first element in Series. \n",
    "df[\"Subject\"].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTHTS87Ddwtg"
   },
   "source": [
    "How would you select only the Imprint column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUUejw7JpDDv"
   },
   "outputs": [],
   "source": [
    "df[\"Imprint\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97qjsM5jpKjT"
   },
   "source": [
    "To more practical purposes, let's move on to selecting subsets of data that meet particular conditions. This is something we might actually want to do in working with a dataset, either as you're exploring it, or to do clean-up or analysis operations on some subset of the rows. You might already do something like this in Excel or Google Sheets with filters. \n",
    "\n",
    "To answer the question \"How many books by Isaac Newton are in this collection?\"  we need to create a test that returns True if the Creator is the value, \"Newton, Isaac\". \n",
    "\n",
    "First, let's just look at the column and test a condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwu5RoBqqNsC"
   },
   "outputs": [],
   "source": [
    "df[\"Creator\"] == \"Newton, Isaac\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "860CIlIjqezl"
   },
   "source": [
    "This returns a series of Trues and Falses. So not very valuable by itself. However, we can use this as a \"mask\" or filter on the entire DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vKoHrM-Zezb"
   },
   "outputs": [],
   "source": [
    "df[df[\"Creator\"] == \"Newton, Isaac\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij3NWTFRqh7h"
   },
   "source": [
    "## Exercise 2.1\n",
    "\n",
    "1. Create a subset of volumes where language is Latin.\n",
    "\n",
    "2. Create a subset of volumes where the language is Latin AND the author is Isaac Newton. \n",
    "Hints: \n",
    "* Put each condition in parentheses. \n",
    "* In pandas, you use the `|`and `&` characters to represent OR and AND. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klChT7iVrRDb"
   },
   "outputs": [],
   "source": [
    "# Answer to exercise 1:\n",
    "df[df[\"Language\"] == \"Latin\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHQkqzuRr72x"
   },
   "outputs": [],
   "source": [
    "# Answer to exercise 2:\n",
    "df[(df[\"Language\"] == \"Latin\") & (df[\"Creator\"] == \"Newton, Isaac\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSC45wzeu3XQ"
   },
   "source": [
    "Text datasets may not have a lot of repeated exactly equal values so you may find it more useful to look at strings that start with or contain particular substrings. To test whether a string contains another substring, we can use a different sort of syntax than the equality test we used before. In pandas, you can use the `.str.contains()` method. to test whether a string contains a pattern or regular expression. \n",
    "\n",
    "(This is where pandas syntax starts to feel particularly weird.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkNkRx2rX9_p"
   },
   "outputs": [],
   "source": [
    "df[df[\"Imprint\"].str.contains(\"London\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwmAlHYOxhxw"
   },
   "source": [
    "If you looked closely at the dataset, however, you might notice that some of the Imprint statements have publication locations that are not in English. For example, London also shows up as Londres and Londini. We can use the `|` within the pattern string to OR together several possible substring patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhY0UymuxiO_"
   },
   "outputs": [],
   "source": [
    " df[df[\"Imprint\"].str.contains(\"London|Londres|Londini\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4xzK-S4O3vu"
   },
   "source": [
    "Our dataset is up to 181 rows now. \n",
    "\n",
    "If we wanted to create a new dataframe with just hte results of this filtering, we can assigned the result of the expression to a new variable, for example, `df2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJy_2xASWaS6"
   },
   "outputs": [],
   "source": [
    "df2 = df[df[\"Imprint\"].str.contains(\"London|Londres|Londini\")]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NemfuvQ3xwtA"
   },
   "source": [
    "## Categorical data and plots\n",
    "\n",
    "One reason to explore your data with pandas is that you can take the results of your filtering and clean-up and make plots for quick analysis. \n",
    "\n",
    "If we wanted to look at the distribution of different languages in the dataset, we can use pandas to get the number of times each language appears and then plot that as a bar chart. Language is a categorical variable, which is why a bar chart is useful. It counts the frequency of distinct categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NENd79bRk1m"
   },
   "outputs": [],
   "source": [
    "#Categorical data: can counts frequency of values.\n",
    "\n",
    "lang_counts = df[\"Language\"].value_counts()\n",
    "lang_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBnq7pDQQMTg"
   },
   "source": [
    "Note that sort_values returns the values in order, with the highest value first. \n",
    "\n",
    "Something to note about lang_counts is that it is a Series (not a DataFrame), and the index is the language name *not* a numeric index like we got when we were looking at columns earlier. Instead, it's the value being counted. We can access a particular language's count by index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzAHoPwaQUg5"
   },
   "outputs": [],
   "source": [
    "type(lang_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMltFyAarg-6"
   },
   "outputs": [],
   "source": [
    "lang_counts[\"Spanish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvnpSFSOrtVT"
   },
   "outputs": [],
   "source": [
    "lang_counts.loc[\"Spanish\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZG33aHOr41Q"
   },
   "source": [
    "## Make and Save Plots\n",
    "\n",
    "Pandas makes it easy to create plots and data visualizations. We can make a simple plot by adding .plot() to any DataFrame or Series object that has appropriate numeric data. \n",
    "\n",
    "(In the case of our language data, the numeric data is the counts of each language value.)\n",
    "\n",
    "We specify the title with the title= parameter and the kind of plot by altering the kind= parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePM6jFvJPiyl"
   },
   "outputs": [],
   "source": [
    "lang_counts.plot(kind=\"bar\", title=\"Number of Volumes by Language\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ5Jr5XBw7fm"
   },
   "source": [
    "The number of volumes per language drops off quickly, so let's adjust the plot to only show the top ten most frequent languages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsTHR5OAyPAr"
   },
   "outputs": [],
   "source": [
    "lang_counts[:10].plot(kind=\"bar\", title=\"Number of Volumes by Language\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyk4wEVBye50"
   },
   "source": [
    "Some more options to try in order to improve the formatting. It's easier to see what is available if we use a slightly different, more specific syntax with the bar() method.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.bar.html?#pandas.DataFrame.plot.bar\n",
    "\n",
    "and for kwargs:\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html#pandas.DataFrame.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WerV4NzuXRoX"
   },
   "outputs": [],
   "source": [
    "# color: str, array-like, or dict, optional. The color for each of the DataFrame’s columns.\n",
    "# figsize: a tuple (width, height) in inches\n",
    "# rot = rotation of ticks. Note that it messes up the long tickname.\n",
    "\n",
    "lang_counts[:10].plot.bar(title=\"Number of Volumes by Language\", \n",
    "                          color=[\"green\", \"blue\", \"red\"],  \n",
    "                          figsize=(10, 5), rot=45, grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0HlewNYBHyA"
   },
   "source": [
    "#### Exercise 2.2 #####\n",
    "\n",
    "1. Determine who the most frequent authors (or \"Creators\") are in the set. \n",
    "\n",
    "2. Use the same approach as what we used with language and make a chart showing the top 25 authors. Experiment with the formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAVRqX2cBGb9"
   },
   "outputs": [],
   "source": [
    "author_counts = df[\"Creator\"].value_counts()\n",
    "author_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNReOV71Bd8E"
   },
   "outputs": [],
   "source": [
    "# remove the first one that's Columbian College by changing slice\n",
    "# Add a title and figsize\n",
    "author_counts.iloc[1:25].plot.bar(figsize=(10,5), \n",
    "                                  title=\"Volumes by Author\\n\", \n",
    "                                  color=\"green\", rot=\"80\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZCTibpF-nlr"
   },
   "source": [
    "## Regular expressions in pandas\n",
    "\n",
    "There are many ways to apply regular expressions in pandas. We'll look at one approach, but this is not only way.  \n",
    "\n",
    "Let's say we wanted to get the year from the \"Imprint\" column, We can use the pandas `str.extract()` method to use a regular expression on a column. Earlier, we used `str.contains` to test whether a value contained a particular substring. \n",
    "\n",
    "`str.extract()` will allow us to use a pattern in the form or a regex to match and then grab that matching substring. \n",
    "\n",
    "Here's one of the regualr expressions we came up with earlier to access a four-digit year#\n",
    "\n",
    "`(.+)\\s?[:,]\\s(.+)[,;]\\s(\\d{4})`\n",
    "\n",
    "We can apply that pattern to a whole column. \n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html?#pandas.Series.str.extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqIxMCNq3GgG"
   },
   "outputs": [],
   "source": [
    "df[\"Imprint\"].str.extract('(.+)\\s?[:,]\\s(.+)[,;]\\s(\\d{4})')\n",
    "\n",
    "# Note that not all Imprints will match and the result in those cases is NaN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt2UA7NYFJ-4"
   },
   "source": [
    "So why are there three columns in the returned DataFrame? The pattern we used has three capture groups, since we were trying to grab different parts initially. \n",
    "\n",
    "So, we could access just that last column, labeled \"2\" and make that our Year column, OR we could edit the statement to only have one capture group (here in cell below). \n",
    "\n",
    "`.+\\s?[:,]\\s.+[,;]\\s(\\d{4})`\n",
    "\n",
    "`df[\"Imprint\"].str.extract(\".+\\s?[:,]\\s.+[,;]\\s(\\d{4})\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0pfzB76FJHm"
   },
   "outputs": [],
   "source": [
    "df[\"Year\"] = df[\"Imprint\"].str.extract('(.+)\\s?[:,]\\s(.+)[,;]\\s(\\d{4})')[2]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLS7a9WiTHUO"
   },
   "source": [
    "**END HERE.** \n",
    "\n",
    "Next up: more using these skills with another dataset for text analysis. \n",
    "\n",
    "**Extra content**\n",
    "What's below is extra code related to alternative ways to work with regular expressions and using `.apply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2DZqCgRU8jB"
   },
   "outputs": [],
   "source": [
    "# Use regular expressions to extract the date from the first column and make a new column with that data. \n",
    "\n",
    "# Option 1 (not preferred?) is to use re.compile and then a function to apply it. \n",
    "import re\n",
    "title = \"Philadelphia, Bradford and Inskeep, 1808-14.\"\n",
    "pattern = re.compile(\"(.+)\\s?[:,]\\s(.+)[,;]\\s(\\d{4})\")\n",
    "\n",
    "result = re.search(pattern, title)\n",
    "print(result)\n",
    "print(result[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nx-gl-2wXYup"
   },
   "outputs": [],
   "source": [
    "# alternatively use re.compile on the column. May be too much for this lesson. \n",
    "\n",
    "def get_year_without_try(title):\n",
    "    pattern = re.compile(\"(.+)\\s?[:,]\\s(.+)[,;]\\s(\\d{4})\")\n",
    "    result = re.search(pattern, title)\n",
    "    return result[3]\n",
    "\n",
    "\n",
    "def get_year(title):\n",
    "    pattern = re.compile(\"(.+)\\s?[:,]\\s(.+)[,;]\\s(\\d{4})\")\n",
    "    result = re.search(pattern, title)\n",
    "    try:\n",
    "        return result[3]\n",
    "    except:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqT3RUFDYBgK"
   },
   "outputs": [],
   "source": [
    "# This will raise an error because of how None can not be accessed with an index\n",
    "\n",
    "df[\"Year_Re\"] = df[\"Imprint\"].apply(get_year_without_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3pvv_D4YQIe"
   },
   "outputs": [],
   "source": [
    "# note last value is None\n",
    "\n",
    "df[\"Year_Re\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIUAsEnT6qMY"
   },
   "outputs": [],
   "source": [
    "df[\"Year_Re\"] = df[\"Imprint\"].apply(get_year)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xN4iTa4XNzE"
   },
   "source": [
    "From Python docs: Match objects always have a boolean value of True. Since match() and search() return None when there is no match, you can test whether there was a match. \n",
    "\n",
    "If search() finds multiple matches, only the last one is accessible. \n",
    "\n",
    "https://docs.python.org/3/library/re.html#match-objects"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "python_part_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
