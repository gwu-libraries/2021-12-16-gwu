{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Carpentry: Tools for Humanists\n",
    "\n",
    "## Python Lesson\n",
    "\n",
    "### Part 3: Working with HathiTrust Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    " - Explore HathiTrust extracted-feature datasets for textual analysis\n",
    " - Work with core elements for the computational analysis of texts, including token counts, parts of speech (POS) tags, and collocations\n",
    " - Practice using the pandas library in Python to query, sort, and group datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. HathiTrust as a data source\n",
    " - Collection of digitized material from major research libraries\n",
    " - Full text available for _selected_ works in the public domain \n",
    " - Permits _non-consumptive_ uses of works in copyright\n",
    "   - The [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) model\n",
    "\n",
    "##### Exercise\n",
    "Visit the [HathiTrust catalog](https://www.hathitrust.org/) and look for the edition of Newton's _Opticks_ published by William Innys in 1730. \n",
    "\n",
    "##### Notes\n",
    "https://catalog.hathitrust.org/Record/007709419?type%5B%5D=all&lookfor%5B%5D=newton%20opticks&ft=ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Getting extracted features\n",
    " - Getting the volume ID\n",
    " - installing the feature reader library\n",
    " - using the `Volume` module to load the dataset into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the `Full View` link. The volume ID can be found in the URL of the volume, between the `id=` and the `&view` parts of the URL: `https://babel.hathitrust.org/cgi/pt?id=uc2.ark:/13960/t3ws8zp9j&view=1up&seq=3&skin=2021`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_id = 'uc2.ark:/13960/t3ws8zp9j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install a specific Python library in order to retrieve the extracted features for this volume. To do so, we use the Python tool `pip`. \n",
    "\n",
    "Note the exclamation point: `pip` is a shell command, not a Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install htrc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = Volume(newton_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic metadata -- we use the \"dot\" notation to access _attributes_ of a complex Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.pub_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Working with tokens and token counts\n",
    " - A _token_ is a quantifiable unit of text\n",
    " - For English text, it's typical to treat the following as tokens:\n",
    "   - Individual words (separated by white space and/or punctuation)\n",
    "   - Punctuation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a pandas DataFrame showing the frequency of each token in the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts = vol.term_volume_freqs(page_freq=False)\n",
    "term_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Compare these token counts with those you obtained in our Python lesson #1. How does your tokenization differ from what's represented here?\n",
    "\n",
    "##### Notes\n",
    " - We only split on whitespace, which included punctuation marks as part of tokens\n",
    " - Including part-of-speech tags distinguishes between homonyms, e.g. \"that\" as conjunction (\"the book that I read\") and determiner (\"that book is a good one\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts.loc[term_counts.token == 'that']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Use the `term_counts` DataFrame to find out how many times the token \"light\" appears in this text. \n",
    "\n",
    "If it helps, you can look up the meaning of a `pos` tag on the [Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) page.\n",
    "\n",
    "##### Notes\n",
    " - Use `.loc` to filter on the condition where the `token` column matches a certain string. \n",
    " - Looking for lowercase \"light\" returns only adjectival uses. \n",
    " - Because of the period in which this was printed, the substantive uses are capitalized.\n",
    " - OCR quality makes a difference, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts.loc[term_counts.token == 'light']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts.loc[term_counts.token == 'Light']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `str.startswith` method to find words similar to \"light.\" Notice some that seem like OCR errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts.loc[term_counts.token.str.startswith('Ligh')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Doing more with tokens: collocation\n",
    " - Using a cleaner text\n",
    " - Getting unique values from a column\n",
    " - Grouping by part of speech\n",
    " - Filtering by part of speech\n",
    " - Grouping by page\n",
    " - Finding collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a cleaner version of Newton's text. [This edition](https://catalog.hathitrust.org/Record/100956962) was published in 1931.\n",
    "\n",
    "Note that the full-text is not available for view. But we can still access the extracted feartures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1931 = Volume('umn.319510005360571')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll use the `tokenlist` method to get a DataFrame showing token count and type by page.\n",
    "\n",
    "The `case` parameter will perform a _casefold_ on the tokens, allowing us to ignore the difference between capitalized and uncapitalized terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol1931.tokenlist(case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `reset_index` method to make the DataFrame easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokens.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "Take a moment to explore the structure of this DataFrame. How does it differ from the previous one?\n",
    "\n",
    "##### Notes\n",
    " - Includes token counts for each page/section of the page\n",
    " - Can't get a total count of token frequency simply by filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.loc` indexer to limit the DataFrame to just rows containing the token(s) we are interested in. Then we can access the token column and call the `unique` method to get a list of the _unique_ values in that column. \n",
    "\n",
    "Note that the token column is now called `lowercase` because we passed the `case=False` argument to the `tokenlist` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_words = df.loc[df.lowercase.str.startswith('ligh')]\n",
    "light_words.lowercase.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same method can help us see what kinds of parts of speech are present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pos.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique` method doesn't tell us how frequently these occur. For that, we need to summarize all the occurences across all pages and all tokens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing aggregations in pandas\n",
    " - Write out a section of the DataFrame on the board, showing multiple pages\n",
    " - Walk through what a group operation would look like (for grouping by pages)\n",
    " - Sorting the DataFrame by a different column changes the ordering of the elements\n",
    " - The `groupby` function as implicit iteration\n",
    " - Steps:\n",
    "   - Identify the column (or columns) that holds the groups => in this case, it's `pos`\n",
    "   - Create a groupby object using that column\n",
    "   - Identify the column you want to summarize (it should be numeric)\n",
    "   - Apply an aggregate function to that column\n",
    " - Time permitting, show `groupby` \"under the hood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: it's not necessary to sort the DataFrame before using groupby\n",
    "# We're doing it here just for illustration\n",
    "df.sort_values(by='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_groups = df.groupby('pos')\n",
    "pos_groups['count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Under the hood of the `groupby` method\n",
    "\n",
    "- The `groupby` method creates an object that consists of Python tuples (a data structure similar to lists)\n",
    "- The first element in each tuple is the label for each group -- in this case, the part of speech\n",
    "- These labels are equivalent to the result of `df.pos.unique()`\n",
    "- The second element in each tuple is a slice of the original DataFrame corresponding to rows where that label is in the groupby column\n",
    "- The latter is equivalent to `df.loc[df.pos==label]` where `label` has the value of the first element in the tuple. \n",
    "- Applying an aggregation function -- like `sum` -- to a column on a `GroupBy` object performs that function _on each slice of the original DataFrame_ and returns a new pandas `Series`.\n",
    "- In the Series, the index consists of the labels, and the values consist of the results of the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(pos_groups)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4\n",
    "Can you recreate the kind of token count we got from `term_volume_freqs` by using the `groupby` method on the page-level DataFrame?\n",
    "\n",
    "##### Notes\n",
    " - `df.groupby('lowercase')['count'].sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collocations\n",
    "A popular method of text analysis consists in seeing which words occur in close proximity to other words. In fact, many sophisticated techniques, such as word vectorization, rely on a version of collocation. \n",
    "\n",
    "With collocation, we treat the text has having more structure than a simple \"bag of words.\"\n",
    "\n",
    "We can perform a kind on our extracted features dataset, using the _page_ as the measure of collocation. In other words, which tokens occur most frequently with which other tokens on the same page of Newton's text? \n",
    "\n",
    "We'll stick to the token `light` to make things easier to implement. Which other tokens appear on the same page as the token `light`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also restrict the kinds of tokens we're including by part of speech. Otherwise, we'll end up with a lot of words like \"the\" and \"of\" in the top collocations.\n",
    "\n",
    "Another approach to doing this is to use [stop words](https://en.wikipedia.org/wiki/Stop_word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a list of parts of speech. Let's stick to singular and plural nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['NN', 'NNS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `isin` method with `.loc` to filter our DataFrame of tokens to just those whose parts of speech are in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_df = df.loc[df.pos.isin(pos)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to filter our DataFrame to just those _pages_ where the token \"light\" appears. Note that it's not sufficient to filter like this: `noun_df.loc[noun_df.token == 'light']`. That would exclude all rows where the token was anything else _but_ \"light.\" But those are the tokens we are trying to count!\n",
    "\n",
    "What we want to do is filter by _groups_, where each group consists of all the tokens on a particular page. \n",
    "\n",
    "We can do that with a special `filter` method on a `GroupBy` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions that act on other functions\n",
    "\n",
    "We've used functions and methods in Pythons with arguments. The `groupby` method is one: as an argument, it takes the name of a DataFrame column or a list of column names.\n",
    "\n",
    "But Python functions can also take _other functions_ as their arguments. If you have taken a course in logic or calculus, this is equivalent to the idea of `F(G(x))`, where `F` and `G` are both well-defined mathematical or logical expressions or equations.\n",
    "\n",
    "What we need is a function that will take a DataFrame slice as its argument and return just those rows where _at least one of the rows contains the token \"light_.  \n",
    "\n",
    "What would such a function look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example page of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page100 = noun_df.loc[noun_df.page==100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page happens to contain the word \"light.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we tell without looking? One way would be to write this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page100.lowercase == 'light'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows here evaluate to `False`, because the token in that row is not \"light.\" But for the row that contains \"light,\" the result is `True`. \n",
    "\n",
    "But we want a single result -- `True` or `False` -- depending on whether ANY row in this DataFrame slice contains the token \"light.\" Handily, there's a Python function called `any` for just these situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(page100.lowercase == 'light')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can write a function to perform this test on an arbitrary slice of our DataFrame.\n",
    "\n",
    "The argument to our function is a DataFrame slice. The return value will be `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_light(df_slice):\n",
    "    return any(df_slice.lowercase == 'light')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we group by the `page` column and pass our `has_light` function to the `filter` method. Note that we are not _calling_ our `has_light` function -- there are no parentheses. The `filter` method will take care of that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_light = noun_df.groupby('page').filter(has_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will include rows for _just those pages_ that contain the token \"light.\"\n",
    "\n",
    "Now we can group a second time, this time by token (`lowercase`), aggregating over the `count` column. \n",
    "\n",
    "Finally, we can use the `sort_values` method with an `ascending` argument to show us the most frequent occurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_light.groupby('lowercase')['count'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5\n",
    "Find a volume in HathiTrust that interests you. See if you can replicate the steps above to do the following: \n",
    " 1. Load the extracted features dataset.\n",
    " 2. Find the most common tokens.\n",
    " 3. Find the most common noun tokens (or some other part of speech).\n",
    " 4. Pick a particular token and find which other tokens occur most commonly with it on the same page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
